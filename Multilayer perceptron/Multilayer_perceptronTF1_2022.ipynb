{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salvadorglezf/Classic-deep-learning-exercises/blob/main/Multilayer_perceptronTF1_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M5Lj0b9jNaG"
      },
      "source": [
        "## Tarea 1 Red neuronal Perceptrón multicapa  en Tensorflow 1\n",
        "\n",
        "#Gonzalez Flores Jesus Salvador\n",
        "\n",
        "En esta tarea se realizó un clasificador de prendas de ropa usando el dataset MNIST Fashion el cual consiste en un conjunto de entrenamiento de 60,000 imágenes de entrenamiento y 10,000 imágenes de prueba a blanco y negro de 28X28 en 10 categorías diferentes. Se implemento y ajusto una red neuronal perceptrón multicapa en Tensorflow 1 con una precisión del 81%.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la implementación del modelo se usó Tensorflow que es una plataforma de código abierto de machine learning, el cual contiene implementaciones de distintas funciones de costo, optimizadores, funciones de transferencia y distintas herramientas que nos facilitaran la implementación de una arquitectura de aprendizaje profunda, con la facilidad que podremos enfocarnos más en el diseño del algoritmo.\n"
      ],
      "metadata": {
        "id": "9SmJ5G21ff4w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMHueSlQkIeY",
        "outputId": "4989e1f0-d1fe-4d6c-a4a0-91c4695faffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Se uso además la librería de numpy, matplotlib \n",
        "para el uso de funciones que nos faciliten el procesado de datos como shuffle, reshape  y graficar los datos."
      ],
      "metadata": {
        "id": "5yystSypfqN0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eH3DG58ijNaH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_r9CetUjNaM"
      },
      "source": [
        "## Lectura del conjunto de datos "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c_aVHNInjNaM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
        "fashion_mnist = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGPrEVaNjNaS"
      },
      "source": [
        "## Separando el conjunto de datos en Entrenamiento y prueba "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se inicia acondicionado los datos, y se visualiza como están distribuidos."
      ],
      "metadata": {
        "id": "84ufH6rdkvO2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xISFxwdKjNaT"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test)=fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhZ32T8levA5",
        "outputId": "14eefc45-a05f-4a4f-f18d-79bddc05e9cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LE8ovpUgGlOk"
      },
      "outputs": [],
      "source": [
        "imagendemo=np.reshape(x_train[24,:],(28,28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-qIEpOomD2eZ"
      },
      "outputs": [],
      "source": [
        "train_labels=tf.keras.utils.to_categorical(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKyK87bnRvqu",
        "outputId": "b2840ead-697a-4c63-885f-a98dd3c00ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print(type(train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyEr3eK5Y81L",
        "outputId": "1c3f245b-2a93-4b51-d780-eb423aa74d1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "x_train.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HgclF9djNaW"
      },
      "source": [
        "Analizando el conjunto de Fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b7cbCu5yjNab"
      },
      "outputs": [],
      "source": [
        "imagendemo=x_train[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "0DIV1V6UjNae",
        "outputId": "9de008bf-3f3d-4922-fb93-54ab7fefe161"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f945a892910>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(imagendemo,cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn5aH2NdjNai"
      },
      "source": [
        "Las etiquetas numéricas pueden ser transformadas al nombre de la clase correspondiente usando el siguiente diccionario "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "spo3e07vjNaj"
      },
      "outputs": [],
      "source": [
        "label_dict = {\n",
        " 0: \"T-shirt/top\",\n",
        " 1: \"Trouser\",\n",
        " 2: \"Pullover\",\n",
        " 3: \"Dress\",\n",
        " 4: \"Coat\",\n",
        " 5: \"Sandal\",\n",
        " 6: \"Shirt\",\n",
        " 7: \"Sneaker\",\n",
        " 8: \"Bag\",\n",
        " 9: \"Ankle boot\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1-fvDkajNam",
        "outputId": "8c765535-8eed-423c-9457-b723ab0889e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'T-shirt/top',\n",
              " 1: 'Trouser',\n",
              " 2: 'Pullover',\n",
              " 3: 'Dress',\n",
              " 4: 'Coat',\n",
              " 5: 'Sandal',\n",
              " 6: 'Shirt',\n",
              " 7: 'Sneaker',\n",
              " 8: 'Bag',\n",
              " 9: 'Ankle boot'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "label_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZgS8lcxCuJwI",
        "outputId": "973c459b-9541-4754-938c-6b2f02696043"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ankle boot'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#Corresponding label\n",
        "label_dict[y_train[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f1oeYtDGu-Sc"
      },
      "outputs": [],
      "source": [
        "#Podemos ver la imagen con sus valores de intensidad de gris\n",
        "def image_matrix(img):\n",
        "    print('\\n'.join([''.join(['{:4}'.format(int(round(item*255))) for item in row]) \n",
        "      for row in img]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncrIQ2lcvAPB",
        "outputId": "5d122e01-6bd3-468f-fc4f-a57fa131c71d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0   0   0   0 255   0   0   0   0561022440479404386033660318753595550745364652295   0   0   0 255   0   0   0   0   0\n",
            "   0   0   0 255   0   0510033405507455253049980515106171065025650256375056610502355253047940321304335   0   0   0   0   0   0\n",
            "   0   0   0 255   089255457048705466654539044625428403825041310405453876040290456454666548195497254717520910   0   0   0   0   0\n",
            "   0   0   0   0   04335048450438604513544880436054309541310395253774039270430954437044625446254513546665479403060   0   0   0   0\n",
            "   0   0   0   0637549470459004539044370469204768548195476854692046155481955100050235492154845045390446254947022950   0   0   0   0\n",
            "   0   0   0   01071055590487055023553040520205380553295535505406053805545705482554315545705380553805487055100040290   0   0   0   0\n",
            "   0   0   0   02244056355548255533555845538054717538250300902728525245224402116522950344255406051765527855584543095   0   0   0   0\n",
            "   0   0   0   0   06885300904131010200   0   0   02550484571409945119859180   0   051765586505610051765   0   0   0   0\n",
            "   0   0   0   035190346801810517595137705508055335517654692042840415654131041565453905635547430969066301785   0   0   0   0   0\n",
            "   0   0   0   017085341703927057120328951683020655298353289532640336603493533405328952193018615400353850534170550804590   0   0   0\n",
            "   0   0   0   05176550490438604666552530650256502563750619656120060945599256069062220650256069046920408002193024990   0   0   0   0\n",
            "   0   0   0   0311104794057120385052677532385247352550026775290702983529835288152626524990283053621064770487056502512495   0   0   0\n",
            "   0   0   0   041565456455100024225392705049050235510005100050490502355049050745515105100044880219305253040035413102550   0   0   0\n",
            "   0   0   0   050235512555839518105367204947046155466654564546410459004564545900484504717550235193805584547175512558670   0   0   0\n",
            "   0   0   0   0507454921557630147903927048960469204768546920474304692047175466654896048705510001428055845517655278515300   0   0   0\n",
            "   0   0   0   051255494705712010455415654845047430474304692047175466654717545390484504947051510841553805510005253018615   0   0   0\n",
            "   0   0   0   05125550235566104335438604845047430476854641047430471754768545900476854921551510663054060515105176519380   0   0   0\n",
            "   0   0   0   0510005023556865   0451354819546920471754539046920466654692045900466654819551765892549980517655176521420   0   0   0\n",
            "   0   0   0   0510005023556865   04717547685471754768545900469204641046665453904641046665522751122040545527855125521675   0   0   0\n",
            "   0   0   0   0476855049057375   04947047940469204717545900466654666546920461554615545135525301173032895538055100022440   0   0   0\n",
            "   0   0   01530474305100053805   05074548195469204692047175464104666546920471754641044625522751275024735550805023523715   0   0   0\n",
            "   0   0   01275471755202046920   05151047940464104641046665466654692046410459004641044370515101606515045561004998023970   0   0   0\n",
            "   0   0   01275469205253040035   0520204768547685481954896048450484504870548450476854666551510198908925566105023524225   0   0   0\n",
            "   0   0   01275466655304032385   0502354233039015379953799537230377403799538250385054029048705229502040568654972525245   0   0   0\n",
            "   0   0   01530469205304029070   052020441154105545900448804386044115441154437044880413105151029325   0583955074526775   0   0   0\n",
            "   0   0   02295453905202029325   030855344252907029835290702907029835300903034529835288153748516065   0573754998027285   0   0   0\n",
            "   0   0   04590459005253033405   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0571205023531365   0   0   0\n",
            "   0   0   0   0359553850519380   0 255 255   0   0   0   0   0   0   0   0   0   0   0   0339154258518615   0   0   0\n"
          ]
        }
      ],
      "source": [
        "image_matrix(imagendemo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr0doBlEjNap"
      },
      "source": [
        "# Suerte!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct3EG7keTxJr"
      },
      "source": [
        "# Función para generar batch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se escribió una función que separa en lotes los datos de entrenamiento con sus respectivas etiquetas y aplica un shuffle antes de devolverlos."
      ],
      "metadata": {
        "id": "bZAhVDmklSHF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6kFO2QVZTwoL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def next_batch(iter, data, labels,index):\n",
        "\n",
        "    data_shuffle = data[iter:index]\n",
        "    labels_shuffle = labels[iter:index]\n",
        "  \n",
        "\n",
        "    #perm = np.arange(len(data_shuffle))\n",
        "    #np.random.shuffle(perm)\n",
        "  \n",
        "  \n",
        "    #data_shuffle = data_shuffle[perm]\n",
        "    #labels_shuffle = labels_shuffle[perm]\n",
        "    \n",
        "\n",
        "    return data_shuffle, labels_shuffle\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "l4USg71NeS2J"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLzvrDRXvUz9"
      },
      "source": [
        "# **Declarando la arquitectura**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Se modifico el número de capas y de neuronas en cada capa, dado que en la experimentación del modelo se observó que si la red es profunda el error es muy alto y decrece muy lento. Si la red posee pocas capas y muchos nodos en la capa de codificación y salida la precisión no supera el 9%. Así que se decidió contar con una arquitectura pequeña.\n",
        "\n",
        "Se uso como función de activación la función Relu, no fue necesario probar con otra función.\n",
        "\n",
        "Se uso como función de costo la entropía Softmax cruzada toma como parámetros las probabilidades sin normalizar que devuelve la red neuronal y las etiquetas verdaderas, esto indica la diferencia entre la estimación de distribución de probabilidades de la red y la distribución de probabilidades verdadera.\n",
        "\n",
        "A esta función de costo se le aplica la función reduce_mean, el cual calcula el promedio de los elementos del vector.\n",
        "\n",
        "Para el optimizador se usó Adaptive Moment Estimation (ADAM) con un tamaño de paso de 0.001, el cual se observó en este modelo que sí se modificaba el tamaño de paso la precisión avanzaba muy lento con respecto del número de épocas, así que se decidió mantener el tamaño de paso fijo.\n"
      ],
      "metadata": {
        "id": "vlbsSZM-mHmX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is6sU7HgvdeV"
      },
      "source": [
        "Generando función"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0n-_0mxcRpmz"
      },
      "outputs": [],
      "source": [
        "#Aquí sí declaramos la arquitectura nosotros, no como en SKLearn donde sólo la mandamos a llamar\n",
        "def Neural_network_model(\n",
        "    n_nodes_hl1=200,\n",
        "    n_nodes_hl2=500,\n",
        "    n_nodes_hl3=600,\n",
        "    n_nodes_hl4=200,\n",
        "    n_classes=10\n",
        "    ):\n",
        "    # Declarando las entradas y salidas\n",
        "    x=tf.placeholder('float',[None,784]) \n",
        "    #El placeholder es como una especie de memoria reservada que hacemos para la entrada\n",
        "    #Este placeholder sí tiene un tipo de dato estricto, más como en Java o C, no es tan flexible como Python simple\n",
        "    #Le ponemos un None porque no sabemos cuantas entradas habrá en cada paso de la red, recordemos que no pasa dato por dato, \n",
        "    #sólo que cada entrada es del tamaño de la imagen, de 784. \n",
        "    \n",
        "    \n",
        "    y=tf.placeholder('float')\n",
        "    \n",
        "    # Declarando las variables \n",
        "    #Nuestra primera capa oculta se ajusta al tamaño del vector de entrada\n",
        "    #Notese que cada capa se define en un diccionario (esto es opcional) de tf.Variable, con sus pesos y sus bias\n",
        "    #el tf.Variable es una variable que podrá modificarse en momento de ejecución\n",
        "    #se inicializa en algún valor aleatorio, en este caso se usa un aleatorio en una distribución normal\n",
        "    #Hemos dispuesto la arquitectura con las variables n_nodes_hlx de modo que se ajusa el tamaño de las\n",
        "    #capas ocultas con las entradas o parámetros de la función \"Neural_network_model\"\n",
        "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
        "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
        "\n",
        "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
        "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
        "\n",
        "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
        "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
        "\n",
        "    hidden_4_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_nodes_hl4])),\n",
        "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl4]))}\n",
        "\n",
        "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl4, n_classes])),\n",
        "                    'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
        "    \n",
        "    #Si tenemos capas ya entrenadas podemos definirla como una tf.Constant, no variable, porque ya no queremos que esas capas se \n",
        "    #modifiquen\n",
        "\n",
        "    # Declarando la arquitectura\n",
        "    \n",
        "    #Aquí dejamos de forma explicita las operaciones de la propagación hacia adelante\n",
        "    l1 = tf.add(tf.matmul(x,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
        "    #Ya que se hizo la operación, aplicamos la función de activación/Transferencia, en este caso es la función \"relu\"\n",
        "    #Mas adelante hablaremos de las diferents funciones de activación...\n",
        "    l1 = tf.nn.relu(l1)\n",
        "\n",
        "    #Usamos las funciones tf.add y tf.matmul porque ya estan optimizadas\n",
        "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
        "    l2 = tf.nn.relu(l2)\n",
        "\n",
        "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
        "    l3 = tf.nn.relu(l3)\n",
        "\n",
        "    l4 = tf.add(tf.matmul(l3,hidden_4_layer['weights']), hidden_4_layer['biases'])\n",
        "    l4 = tf.nn.relu(l4)\n",
        "\n",
        "    #Pero podemos utilizar los operadores nativos de python\n",
        "    output = tf.matmul(l4,output_layer['weights']) + output_layer['biases']\n",
        "    \n",
        "    # Declarando la funcion de costo y optimizador\n",
        "    #Aquí estamos escogiendo la función de costo o error. \n",
        "    #Estas funciones deben elegirse de acuerdo al tipo de problema que queremos resolver\n",
        "    #para este caso, usamos la entropía cruzada\n",
        "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=output\n",
        "                                                                   , labels=y) )\n",
        "    #También, elegimos el optimizador a utilizar, en este caso se usará uno de los más comunes, \n",
        "    #el AdamOptimizer,\n",
        "    #también, a lo largo del curso revisaremos los optimizadores disponibles \n",
        "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "\n",
        "    #Esta función nos va regresar un diccionario, en este diccionario estamos \n",
        "    #recuperando toda la información importante de la red,\n",
        "    #las entradas, etiquetas  o targets, salidas, la salida de la función de costo \n",
        "    #y el resultado del gradiente u optimizador     \n",
        "    return dict(\n",
        "              x=x,\n",
        "              y=y,\n",
        "              output=output,\n",
        "              cost=cost,\n",
        "              optimizer=optimizer\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIBhNGZTwliT",
        "outputId": "e6893c99-fe56-40e9-9e47-aa4ca9c2d156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-19-7e5b56cc45d3>:70: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cost': <tf.Tensor 'Mean:0' shape=() dtype=float32>,\n",
              " 'optimizer': <tf.Operation 'Adam' type=NoOp>,\n",
              " 'output': <tf.Tensor 'add_4:0' shape=(?, 10) dtype=float32>,\n",
              " 'x': <tf.Tensor 'Placeholder:0' shape=(?, 784) dtype=float32>,\n",
              " 'y': <tf.Tensor 'Placeholder_1:0' shape=<unknown> dtype=float32>}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#Prueba del modelo\n",
        "#Aqui estamos verificando que todo quedo bien declarado\n",
        "Neural_network_model()\n",
        "#Esta función solo sirve para declarar nuestra arquitectura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnND6eokQQyb"
      },
      "source": [
        "## función de entrenamiento  y prueba"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la función que se encarga de entrenar el modelo, se determinó un numero de épocas de 10 muestras y tamaño de lotes de 1000 muestras, esto debido a la relación observada entre el tamaño de lotes y la precisión."
      ],
      "metadata": {
        "id": "ek5UQYWRCkjl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "f-cDNuusRpm8"
      },
      "outputs": [],
      "source": [
        "def train_neural_network(DNN, hm_epochs=10,batch_size=1000):\n",
        "#Esta función recibe el modelo a entrenar, el número de épocas y \n",
        "#el número de datos a propagar en cada paso\n",
        "\n",
        "#Para ejecutar las modelos, en TF1 es necesario indicar en el codigo que se mandaŕa a compilador el código\n",
        "#Por eso, se necesita abrir una session de tensorflow\n",
        "#Python en interpretado, por eso, para mandarlo a CUDA es necesario indicar qué porción sera compilada\n",
        "#Todo lo que queda dentro del tf.Session, se manda a compilador\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        index=0\n",
        "        #Hacemos el ciclo por número de epocas\n",
        "        print(\"--- Etapa de entrenamiento ---\")\n",
        "        for epoch in range(hm_epochs):\n",
        "            epoch_loss = 0\n",
        "            #Hacemos el ciclo por  número de lotes (batches)\n",
        "            index=int(x_train.shape[0]/batch_size)\n",
        "            k=0\n",
        "            for _ in range(batch_size):\n",
        "                #Cargamos en epoch_x y epoch_y nuestros datos y sus etiquetas\n",
        "                epoch_x, epoch_y = next_batch(k, x_train,train_labels,index)#mnist.train.next_batch(batch_size)\n",
        "                \n",
        "                try:\n",
        "                  epo=np.reshape(epoch_x,(60,784))\n",
        "                  index=index+int(x_train.shape[0]/batch_size)\n",
        "                  k=k+int(x_train.shape[0]/batch_size)\n",
        "                  \n",
        "                except ValueError:\n",
        "                  epo=np.reshape(epoch_x,(0,784))\n",
        "                #print(k,index,epo.shape) \n",
        "                #index=index+int(x_train.shape[0]/batch_size)\n",
        "                #k=k+int(x_train.shape[0]/batch_size)\n",
        "                #Le indicamos a la red que formaran parte de su diccionario, \n",
        "                #Estos datos los cargamos en los placeholders que declaramos en el modelo\n",
        "                feed_dict={DNN[\"x\"]: epo, \n",
        "                           DNN[\"y\"]: epoch_y}\n",
        "                #En el sess.run, estamos ejecutando las funciones u operaciones \n",
        "                #que definimos, en este caso son el optimizador y el costo\n",
        "                #también se le dice que necesitará de la salida de la red y las etiquetas de los datos\n",
        "                #además, le damos el diccionario de entrada que declaramos en la linea anterior\n",
        "                #En esta linea se hace el paso de entrenamieno: se propagan los datos, se calcula el costo\n",
        "                #y se optimizan los pesos y los bias (backpropagation)\n",
        "             \n",
        "                _, c, prediction,y   = sess.run([DNN[\"optimizer\"], DNN[\"cost\"]\n",
        "                                                 , DNN[\"output\"], DNN[\"y\"]], \n",
        "                                                feed_dict=feed_dict)\n",
        "                #De este run obtenemos la salida del optimizador que se debe ejcautar para entrenar la red, pero que no usamos \"_\", \n",
        "                #el costo \"c\", la predicción de la red \"prediction\" y las etiquetas de los datos\n",
        "\n",
        "                #Acumulamos en epoch loss el costo calculado de este batch\n",
        "                epoch_loss += c\n",
        "            #La epoca termina cuando se ha propagado el dataset completo\n",
        "            #Imprimimos en pantalla el costo en la época actual\n",
        "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
        "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "            #Este error nos sirve para calcular la exactitud de claisifación\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "            print('Trainning Accuracy:',accuracy.eval())\n",
        "                \n",
        "        \n",
        "        #Prueba con datos nunca antes vistos \n",
        "        print(\"--- Etapa de prueba ---\")\n",
        "        #En este punto la red ya esta entrenada\n",
        "        #Hacemos una propagación de un conjunto de datos que la red no ha visto y obtenemos la salida\n",
        "        #En este sess.run no estamos mandando el optimizador, pecisamente para no modificar los pesos y bias de la red\n",
        "        prediction,y   = sess.run([DNN[\"output\"], DNN[\"y\"]], feed_dict={DNN[\"x\"]:np.reshape(x_test,(10000,784)), DNN[\"y\"]:tf.keras.utils.to_categorical(y_test)})\n",
        "        #Con la salida calculamos un error\n",
        "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "        #Este error nos sirve para calcular la exactitud de claisifación\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "        print('Test Accuracy:',accuracy.eval())\n",
        "        #Esto es el equivalente al predict de SKLearn\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La red se entre y prueba sobre los datos de entrenamiento, resultando una precisión de 81% sobre los datos de prueba."
      ],
      "metadata": {
        "id": "etCCEJr_yXxt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayVvqOAazL13",
        "outputId": "da23cd61-0d2c-44c3-e75a-a8124d360a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Etapa de entrenamiento ---\n",
            "Epoch 0 completed out of 10 loss: 4471791917.875\n",
            "Trainning Accuracy: 0.81666666\n",
            "Epoch 1 completed out of 10 loss: 1572578933.546875\n",
            "Trainning Accuracy: 0.8833333\n",
            "Epoch 2 completed out of 10 loss: 1011530287.90625\n",
            "Trainning Accuracy: 0.8666667\n",
            "Epoch 3 completed out of 10 loss: 721616927.1484375\n",
            "Trainning Accuracy: 0.8666667\n",
            "Epoch 4 completed out of 10 loss: 552605554.9453125\n",
            "Trainning Accuracy: 0.8333333\n",
            "Epoch 5 completed out of 10 loss: 446328475.4482422\n",
            "Trainning Accuracy: 0.85\n",
            "Epoch 6 completed out of 10 loss: 367441084.9355469\n",
            "Trainning Accuracy: 0.81666666\n",
            "Epoch 7 completed out of 10 loss: 303166743.89941406\n",
            "Trainning Accuracy: 0.81666666\n",
            "Epoch 8 completed out of 10 loss: 263191630.49121094\n",
            "Trainning Accuracy: 0.8833333\n",
            "Epoch 9 completed out of 10 loss: 215790053.50927734\n",
            "Trainning Accuracy: 0.8666667\n",
            "--- Etapa de prueba ---\n",
            "Test Accuracy: 0.8055\n"
          ]
        }
      ],
      "source": [
        "DNN=Neural_network_model()\n",
        "train_neural_network(DNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis de resultados\n",
        "\n",
        "La primera versión de este clasificador tenía 190 épocas 100 lotes con 600 ejemplos cada uno, la arquitectura de la red eran 3 capas ocultas con 500 nodos, y las capas de entrada y salida contenían 300 nodos. Con esta arquitectura se conseguía una precisión del 86% sobre los datos de prueba.\n",
        "\n",
        "Se trato de reducir el número de épocas, durante esta experimentación se observó que el clasificador es dependiente del tamaño de lotes, entre más ejemplos contenga el lote el clasificador consigue una mayor precisión si la red no es tan profunda (mayor a 5 capas ocultas y mayor a 500 nodos).\n",
        "\n",
        "Se hace notar que la precisión medida en el conjunto de entrenamiento no refleja directamente la precisión esperada en el conjunto de prueba.\n",
        "\n",
        "Como conclusión es necesario tener una librería que permita variar ciertos parámetros automáticamente y técnicas de entrenamiento en forma similar a la técnica de crossfolding en datos, pero en toda la arquitectura de la red, con la finalidad de agilizar el entrenamiento y prueba con diferentes tipos de datos, esto nos permitiría dedicar más tiempo a conocer el modelo y analizar el problema.\n"
      ],
      "metadata": {
        "id": "FkXi9cM035_q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Multilayer perceptronTF1 2022.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
